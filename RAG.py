import psycopg2
from sentence_transformers import SentenceTransformer
# Thay th·∫ø: import OpenAI
from google import genai 
from google.genai import types # Th√™m ƒë·ªÉ s·ª≠ d·ª•ng c·∫•u h√¨nh
from google.genai.errors import APIError # Th√™m ƒë·ªÉ x·ª≠ l√Ω l·ªói

from credential import config
import os # Th√™m ƒë·ªÉ qu·∫£n l√Ω API Key t·ªët h∆°n

# --- Config ---
TOP_K = 5
query = "ƒêƒÉng t·∫£i n·ªôi dung b√≥c l·ªôt tr·∫ª em c√≥ ƒë√∫ng v·ªõi ch√≠nh s√°ch Facebook?"

# --- K·∫øt n·ªëi DB (Gi·ªØ nguy√™n) ---
DB_NAME = config.DB_NAME
DB_USER = config.DB_USER
DB_PASS = config.DB_PASS
DB_HOST = config.DB_HOST
DB_PORT = config.DB_PORT

# DB connect
conn = psycopg2.connect(
    dbname=DB_NAME,
    user=DB_USER,
    password=DB_PASS,
    host=DB_HOST,
    port=DB_PORT
)
cur = conn.cursor()

# ----------------- Load embedding model (Gi·ªØ nguy√™n) -----------------
# L∆∞u √Ω: N·∫øu b·∫°n mu·ªën d√πng embedding c·ªßa Gemini, b·∫°n s·∫Ω c·∫ßn thay th·∫ø ph·∫ßn n√†y.
# Nh∆∞ng ƒë·ªÉ gi·ªØ nguy√™n logic Retrieval Augment Generation (RAG) hi·ªán t·∫°i, ta gi·ªØ nguy√™n.
model = SentenceTransformer("BAAI/bge-m3")
query_emb = model.encode(query).tolist()

# ----------------- Retrieval (Gi·ªØ nguy√™n) -----------------
cur.execute("""
SELECT id, chunk, embedding <-> %s::vector AS distance
FROM chunks
ORDER BY distance
LIMIT %s;
""", (query_emb, TOP_K))

results = cur.fetchall()
context = "\n".join([r[1] for r in results])
conn.close()

# ----------------- Gemini API (Ph·∫ßn thay ƒë·ªïi ch√≠nh) -----------------

# try:
#     client = genai.Client()
# except Exception as e:
#     print(f"L·ªói kh·ªüi t·∫°o Gemini Client: {e}")
#     print("Vui l√≤ng ƒë·∫£m b·∫£o bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p.")
#     exit()

# C·∫•u h√¨nh Generation
generation_config = types.GenerateContentConfig(
    temperature=0.2
)

prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI. D·ª±a tr√™n th√¥ng tin d∆∞·ªõi ƒë√¢y v·ªÅ nh·ªØng ti√™u chu·∫©n v√† ch√≠nh s√°ch c·ªßa Facebook, tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát trong v√≤ng 200 t·ª´. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, h√£y tr·∫£ l·ªùi 'Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan'.

Th√¥ng tin: {context}

C√¢u h·ªèi: {query}
"""
print("üìù Prompt:", prompt)

# try:
#     # G·ªçi Gemini API
#     # S·ª≠ d·ª•ng m√¥ h√¨nh Gemini ph√π h·ª£p, v√≠ d·ª• gemini-2.5-flash
#     resp = client.models.generate_content(
#         model="gemini-2.5-flash", # M√¥ h√¨nh m·∫°nh m·∫Ω, thay th·∫ø gpt-4.1-mini
#         contents=[prompt],
#         config=generation_config,
#     )

#     answer = resp.text
#     print("üí° Tr·∫£ l·ªùi:", answer)

# except APIError as e:
#     print(f"L·ªói g·ªçi Gemini API: {e}")
# except Exception as e:
#     print(f"ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")